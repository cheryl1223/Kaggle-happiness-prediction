{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3695, 108)\n",
      "(3695, 758)\n"
     ]
    }
   ],
   "source": [
    "from preprocess import transform\n",
    "from preprocess import fill_missing\n",
    "from lr import LogisticRegression\n",
    "from naive_bayes import NaiveBayes\n",
    "from sklearn.naive_bayes import MultinomialNB #GaussianNB\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import cross_validation\n",
    "from sklearn import linear_model\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# load training data\n",
    "filename_train = './data/train.csv'\n",
    "train_dataset = transform(filename_train)\n",
    "X = train_dataset['data'].drop(\"UserID\",1)\n",
    "y = train_dataset['target']\n",
    "print(X.shape)\n",
    "X_full = fill_missing(X,strategy = 'mean', isClassified = False)\n",
    "\n",
    "encoder = OneHotEncoder()\n",
    "X_full = encoder.fit_transform(X_full).toarray()\n",
    "print (X_full.shape)\n",
    "X_train = X_full\n",
    "y_train = y\n",
    "\n",
    "\n",
    "X_a, X_b, y_a, y_b = cross_validation.train_test_split(\n",
    "    X_train, y_train, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train the logistic regression classifier\n",
      "Accuracy of Linear regression is  0.65899864682\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cheryl/3320/project/lr.py:12: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-scores))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Linear regression is  0.645466847091\n"
     ]
    }
   ],
   "source": [
    "\n",
    "### use the logistic regression\n",
    "print('Train the logistic regression classifier')\n",
    "lr_model = linear_model.LogisticRegression()\n",
    "lr_model = lr_model.fit(X_a, y_a)\n",
    "lr_model_predict = lr_model.predict(X_b)\n",
    "print(\"Accuracy of Linear regression is \", (y_b == lr_model_predict).sum()/y_b.shape[0])\n",
    "clf_lr = LogisticRegression()\n",
    "clf_lr = clf_lr.fit(X_a, y_a)\n",
    "clf_lr_predict = clf_lr.predict(X_b)\n",
    "print(\"Accuracy of Linear regression is \", (y_b == clf_lr_predict).sum()/y_b.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train the naive bayes classifier\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-5d056dbbd074>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Train the naive bayes classifier'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mclf_nb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNaiveBayes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mclf_nb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf_nb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_a\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_a\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0my_predict_me\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf_nb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_b\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Accuracy of Multinomial NB is \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0my_b\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0my_predict_me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0my_b\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/cheryl/3320/project/naive_bayes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamp_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_class\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m                 \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_y_label\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_log_pik\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_log_pik\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    601\u001b[0m         \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply_if_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 603\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    604\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    605\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/pandas/indexes/base.py\u001b[0m in \u001b[0;36mget_value\u001b[0;34m(self, series, key)\u001b[0m\n\u001b[1;32m   2167\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2168\u001b[0m             return self._engine.get_value(s, k,\n\u001b[0;32m-> 2169\u001b[0;31m                                           tz=getattr(series.dtype, 'tz', None))\n\u001b[0m\u001b[1;32m   2170\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2171\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minferred_type\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'integer'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'boolean'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/index.pyx\u001b[0m in \u001b[0;36mpandas.index.IndexEngine.get_value (pandas/index.c:3557)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/index.pyx\u001b[0m in \u001b[0;36mpandas.index.IndexEngine.get_value (pandas/index.c:3240)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/index.pyx\u001b[0m in \u001b[0;36mpandas.index.IndexEngine.get_loc (pandas/index.c:4279)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/src/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas.hashtable.Int64HashTable.get_item (pandas/hashtable.c:8564)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/src/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas.hashtable.Int64HashTable.get_item (pandas/hashtable.c:8508)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 1"
     ]
    }
   ],
   "source": [
    "### use the naive bayes\n",
    "print('Train the naive bayes classifier')\n",
    "clf_nb = NaiveBayes()\n",
    "clf_nb = clf_nb.fit(X_a,y_a)\n",
    "y_predict_me = clf_nb.predict(X_b)\n",
    "print(\"Accuracy of Multinomial NB is \", (y_b == y_predict_me).sum()/y_b.shape[0])\n",
    "\n",
    "nb_model = MultinomialNB()\n",
    "nb_model = nb_model.fit(X_a,y_a)\n",
    "y_predict = nb_model.predict(X_b)\n",
    "#print(y_predict.shape)\n",
    "print(\"Accuracy of scikit-learn Multinomial NB is \", (y_b == y_predict).sum()/y_b.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train the SVM classifier\n",
      "Accuracy of SVM is  0.691474966171\n",
      "Train the random forest classifier\n",
      "cross validation sklearn_rf_score:  0.679296346414\n"
     ]
    }
   ],
   "source": [
    "## use the svm\n",
    "print('Train the SVM classifier')\n",
    "# svm_model = ...\n",
    "svm_model = SVC(kernel='rbf')\n",
    "svm_model = svm_model.fit(X_a,y_a)\n",
    "y_predict_svm = svm_model.predict(X_b)\n",
    "print(\"Accuracy of SVM is \", (y_b == y_predict_svm).sum()/y_b.shape[0])\n",
    "\n",
    "## use the random forest\n",
    "print('Train the random forest classifier')\n",
    "rf_model = RandomForestClassifier(n_estimators=100,random_state=10)\n",
    "rf_model = rf_model.fit(X_a, y_a)\n",
    "rf_score = rf_model.score(X_b, y_b)\n",
    "print(\"cross validation sklearn_rf_score: \", rf_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(924, 108)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "shapes (924,746) and (758,) not aligned: 746 (dim 1) != 758 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-a686f977b603>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# clf_lr predictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mlr_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf_lr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0mlr_Happy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'Happy'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlr_pred\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mpredsLR\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mUserID\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr_Happy\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/cheryl/3320/project/lr.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0mprobability\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoef_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m0.5\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprobability\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: shapes (924,746) and (758,) not aligned: 746 (dim 1) != 758 (dim 0)"
     ]
    }
   ],
   "source": [
    "## get predictions\n",
    "\n",
    "filename_test = './data/test.csv'\n",
    "test_dataset = transform(filename_test)\n",
    "\n",
    "X_test_data = test_dataset['data']\n",
    "UserID = (X_test_data[X_test_data.columns[0]]).copy()\n",
    "\n",
    "X_test_data = X_test_data.drop(\"UserID\",1)\n",
    "\n",
    "X_test = fill_missing(X_test_data, 'most_frequent', False)\n",
    "\n",
    "encoder = OneHotEncoder()\n",
    "X_test = encoder.fit_transform(X_test).toarray()\n",
    "print (X_test_data.shape)\n",
    "\n",
    "# clf_lr predictions\n",
    "lr_pred = clf_lr.predict(X_test)\n",
    "lr_Happy = pd.DataFrame({'Happy': lr_pred})\n",
    "predsLR = pd.concat([UserID, lr_Happy], axis=1)\n",
    "predsLR.to_csv('./predictions/lr_predictions.csv', index=False)\n",
    "\n",
    "#diff_score = t_lr_model.score(X_test, lr_pred)\n",
    "#print(\"lr_mode v.s. sklearn.lr_model on test data: \", diff_score)\n",
    "\n",
    "# clf_nb predictions\n",
    "nb_pred = clf_nb.predict(X_test)\n",
    "nb_Happy = pd.DataFrame({'Happy': nb_pred})\n",
    "predsLR = pd.concat([UserID, nb_Happy], axis=1)\n",
    "predsLR.to_csv('./predictions/nb_predictions.csv', index=False)\n",
    "\n",
    "# rf predictions\n",
    "rf_pred = rf_model.predict(X_test)\n",
    "rf_Happy = pd.DataFrame({'Happy': rf_pred})\n",
    "predsLR = pd.concat([UserID, rf_Happy], axis=1)\n",
    "predsLR.to_csv('./predictions/rf_predictions.csv', index=False)\n",
    "#res = pd.read_csv('./predictions/rf_predictions.csv')\n",
    "#print(res)\n",
    "\n",
    "# svm predictions\n",
    "svm_pred = svm_model.predict(X_test)\n",
    "svm_Happy = pd.DataFrame({'Happy': svm_pred})\n",
    "predsLR = pd.concat([UserID, svm_Happy], axis=1)\n",
    "predsLR.to_csv('./predictions/svm_predictions.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    X_a, X_b, y_a, y_b = cross_validation.train_test_split(\n",
    "        X_train, y_train, test_size=0.2, random_state=0)\n",
    "    \n",
    "    ### use the logistic regression\n",
    "    print('Train the logistic regression classifier')\n",
    "    lr_model = linear_model.LogisticRegression()\n",
    "    lr_model = lr_model.fit(X_a, y_a)\n",
    "    lr_score = lr_model.score(X_b, y_b)\n",
    "    print(\"cross validation sklearn_lr_score: \", lr_score)   \n",
    "    clf_lr = LogisticRegression()\n",
    "    clf_lr = clf_lr.fit(X_a, y_a)\n",
    "    clf_lr_score = clf_lr.score(X_b, y_b)\n",
    "    clf_lr_predict = clf_lr.predict(X_b)\n",
    "    print(\"cross validation clf_lr_score: \", clf_lr_score)\n",
    "    print(\"-------Wrong prediction of Linear regression is \", (y_b != clf_lr_predict).sum()/y_b.shape[0])\n",
    "\n",
    "    ### use the naive bayes\n",
    "    print('Train the naive bayes classifier')\n",
    "    clf_nb = NaiveBayes()\n",
    "    clf_nb = clf_nb.fit(X_a,y_a)\n",
    "    y_predict_me = clf_nb.predict(X_b)\n",
    "    print(\"-------Wrong prediction of Multinomial NB is \", (y_b != y_predict_me).sum()/y_b.shape[0])\n",
    "\n",
    "    nb_model = MultinomialNB()\n",
    "    nb_model = nb_model.fit(X_a,y_a)\n",
    "    y_predict = nb_model.predict(X_b)\n",
    "    #print(y_predict.shape)\n",
    "    print(\"-------Wrong prediction of Multinomial NB is \", (y_b != y_predict).sum()/y_b.shape[0])\n",
    "    \n",
    "    ## use the svm\n",
    "    print('Train the SVM classifier')\n",
    "    # svm_model = ...\n",
    "    svm_model = SVC(kernel='rbf')\n",
    "    svm_model = svm_model.fit(X_a,y_a)\n",
    "    y_predict_svm = svm_model.predict(X_b)\n",
    "    print(\"-------Wrong prediction of SVM is \", (y_b != y_predict_svm).sum()/y_b.shape[0])\n",
    "    \n",
    "    ## use the random forest\n",
    "    print('Train the random forest classifier')\n",
    "    rf_model = RandomForestClassifier(n_estimators=100,random_state=10)\n",
    "    rf_model = rf_model.fit(X_a, y_a)\n",
    "    rf_score = rf_model.score(X_b, y_b)\n",
    "    print(\"cross validation sklearn_rf_score: \", rf_score)\n",
    "\n",
    "    ## get predictions\n",
    "    filename_test = './data/test.csv'\n",
    "    test_dataset = transform(filename_test)\n",
    "    X_test_data = test_dataset['data']\n",
    "    UserID = (X_test_data[X_test_data.columns[0]]).copy()\n",
    "\n",
    "    X_test_data = X_test_data.drop(\"UserID\",1)\n",
    "    X_test = fill_missing(X_test_data, 'mean', False)\n",
    "\n",
    "    # clf_lr predictions\n",
    "    lr_pred = clf_lr.predict(X_test)\n",
    "    lr_Happy = pd.DataFrame({'Happy': lr_pred})\n",
    "    predsLR = pd.concat([UserID, lr_Happy], axis=1)\n",
    "    predsLR.to_csv('./predictions/lr_predictions.csv', index=False)\n",
    "\n",
    "    #diff_score = t_lr_model.score(X_test, lr_pred)\n",
    "    #print(\"lr_mode v.s. sklearn.lr_model on test data: \", diff_score)\n",
    "\n",
    "    # clf_nb predictions\n",
    "    nb_pred = clf_nb.predict(X_test)\n",
    "    nb_Happy = pd.DataFrame({'Happy': nb_pred})\n",
    "    predsLR = pd.concat([UserID, nb_Happy], axis=1)\n",
    "    predsLR.to_csv('./predictions/nb_predictions.csv', index=False)\n",
    "\n",
    "    # rf predictions\n",
    "    rf_pred = rf_model.predict(X_test)\n",
    "    rf_Happy = pd.DataFrame({'Happy': rf_pred})\n",
    "    predsLR = pd.concat([UserID, rf_Happy], axis=1)\n",
    "    predsLR.to_csv('./predictions/rf_predictions.csv', index=False)\n",
    "    #res = pd.read_csv('./predictions/rf_predictions.csv')\n",
    "    #print(res)\n",
    "\n",
    "    # svm predictions\n",
    "    svm_pred = svm_model.predict(X_test)\n",
    "    svm_Happy = pd.DataFrame({'Happy': svm_pred})\n",
    "    predsLR = pd.concat([UserID, svm_Happy], axis=1)\n",
    "    predsLR.to_csv('./predictions/svm_predictions.csv', index=False)\n",
    "    \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
